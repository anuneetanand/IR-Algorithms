{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preprocessing  \n",
    "---\n",
    "The index.html files are removed and 15 files from the SRE folder are shifted to stories folder. The resulting 467 files are preprocessed before creating the Unigram Inverted Index. Necessary steps are undertaken to clean the document text. The documents' names, original texts and cleaned texts obtained after preprocessing are stored in a pickle file **docs.pkl**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing relevent modules for preprocessing\n",
    "\n",
    "import re\n",
    "import string\n",
    "import codecs\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from substitutions import appos\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Documents : 467\n",
      "Unresolved Documents : []\n"
     ]
    }
   ],
   "source": [
    "# Reading the 467 documents and storing the document name and document text\n",
    "\n",
    "Data = []\n",
    "Unresolved = []\n",
    "\n",
    "for i in Path(\"../stories\").glob(\"*\"):\n",
    "    try:\n",
    "        with codecs.open(i,'r', encoding = 'utf-8', errors = 'ignore') as f:\n",
    "            d = str(i).split('/')[-1]\n",
    "            t =  ' '.join(f.readlines())\n",
    "            Data.append({'doc_name' : d, 'text' : t})\n",
    "    except:\n",
    "        Unresolved.append(str(i))\n",
    "        \n",
    "print(\"Total Documents :\",len(Data))\n",
    "print(\"Unresolved Documents :\",Unresolved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean document text\n",
    "\n",
    "def clean(text):\n",
    "    \n",
    "    # Converting all text to lowercase\n",
    "    text = text.lower()\n",
    "        \n",
    "    # Substituting words with apostrophe with their appropiate phrases\n",
    "    text = ' '.join([appos[word] if word in appos else word for word in word_tokenize(text)])\n",
    "    \n",
    "    # Removing all punctuation and unecessary characters from text\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    \n",
    "    # Removing stopwords from text\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    text = ' '.join([w for w in word_tokenize(text) if not w in stop_words])\n",
    "    \n",
    "    # Lemmatizing\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = ' '.join([lemmatizer.lemmatize(w) for w in word_tokenize(text) if not w in stop_words])\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 467/467 [00:38<00:00, 12.07it/s]\n"
     ]
    }
   ],
   "source": [
    "# Cleaning\n",
    "\n",
    "for doc in tqdm(Data):\n",
    "    doc['cleaned_text'] = clean(doc['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting documents in Alphabetical order\n",
    "\n",
    "Data.sort(key = lambda x:x['doc_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dumping in Pickle File\n",
    "\n",
    "pickle.dump(Data,open('../Dumps/docs.pkl','wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
