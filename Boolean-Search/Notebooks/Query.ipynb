{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Processing\n",
    "---\n",
    "Separate functions have been created for the 5 types of operations: AND, OR, NOT, AND NOT, OR NOT.\n",
    "The index and docIDs are loaded from their respective dumps.\n",
    "The Query is processed from left to right.  \n",
    "\n",
    "**Input Format**    \n",
    "\n",
    "Input 1 : Query Sentence  \n",
    "Input 2 : Operators separted using \", \" without any enclosing square brackets  \n",
    "\n",
    "**Output Format**  \n",
    "\n",
    "Number of documents matched: _int_  \n",
    "Number of comparisons required: _int_  \n",
    "Documents: _List of Document Names_  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing relevent modules for Querying\n",
    "\n",
    "import re\n",
    "import string\n",
    "import codecs\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from substitutions import appos\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import OrderedDict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Pickle Dumps\n",
    "\n",
    "with open(\"../Dumps/index.pkl\",\"rb\") as file:\n",
    "    postings = pickle.load(file)\n",
    "    \n",
    "with open(\"../Dumps/docID.pkl\",\"rb\") as file:\n",
    "    docID = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean document text\n",
    "\n",
    "def clean(text):\n",
    "    \n",
    "    # Converting all text to lowercase\n",
    "    text = text.lower()\n",
    "        \n",
    "    # Substituting words with apostrophe with their appropiate phrases\n",
    "    text = ' '.join([appos[word] if word in appos else word for word in word_tokenize(text)])\n",
    "    \n",
    "    # Removing all punctuation and unecessary characters from text\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    \n",
    "    # Removing stopwords from text\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    text = ' '.join([w for w in word_tokenize(text) if not w in stop_words])\n",
    "    \n",
    "    # Lemmatizing\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = ' '.join([lemmatizer.lemmatize(w) for w in word_tokenize(text) if not w in stop_words])\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing OR operation\n",
    "\n",
    "def OR(postings1, postings2):\n",
    "    \n",
    "    i = 0\n",
    "    j = 0\n",
    "    n = len(postings1)\n",
    "    m = len(postings2)\n",
    "    mergedPostings = []\n",
    "    comparisons = 0\n",
    "    \n",
    "    while i < n and j < m:\n",
    "        comparisons += 1\n",
    "        \n",
    "        if postings1[i] < postings2[j]:\n",
    "            mergedPostings.append(postings1[i])\n",
    "            i += 1\n",
    "        elif postings1[i] > postings2[j]:\n",
    "            mergedPostings.append(postings2[j])\n",
    "            j += 1\n",
    "        else:\n",
    "            mergedPostings.append(postings1[i])\n",
    "            i += 1\n",
    "            j += 1\n",
    "    \n",
    "    while i < n:\n",
    "        mergedPostings.append(postings1[i])\n",
    "        i += 1\n",
    "        \n",
    "    while j < m:\n",
    "        mergedPostings.append(postings2[j])\n",
    "        j += 1\n",
    "\n",
    "    return mergedPostings, comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing AND operation\n",
    "\n",
    "def AND(postings1, postings2):\n",
    "    \n",
    "    i = 0\n",
    "    j = 0\n",
    "    n = len(postings1)\n",
    "    m = len(postings2)\n",
    "    mergedPostings = []\n",
    "    comparisons = 0\n",
    "    \n",
    "    while i < n and j < m:\n",
    "        comparisons += 1\n",
    "        \n",
    "        if postings1[i] < postings2[j]:\n",
    "            i += 1\n",
    "        elif postings1[i] > postings2[j]:\n",
    "            j += 1\n",
    "        else:\n",
    "            mergedPostings.append(postings1[i])\n",
    "            i += 1\n",
    "            j += 1\n",
    "            \n",
    "    return mergedPostings, comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing NOT operation\n",
    "\n",
    "def NOT(postings, totalDocs = 467):\n",
    "    \n",
    "    i = 0\n",
    "    j = 0\n",
    "    n = len(postings)\n",
    "    negativePostings = []\n",
    "    comparisons = 0\n",
    "    \n",
    "    while j < totalDocs:\n",
    "        comparisons += 1\n",
    "        \n",
    "        if i<n and postings[i]==j:\n",
    "            i+=1\n",
    "        else:\n",
    "            negativePostings.append(j)\n",
    "        j+=1\n",
    "    \n",
    "    return negativePostings, comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing AND NOT operation\n",
    "\n",
    "def ANDNOT(postings1, postings2):\n",
    "    \n",
    "    i = 0\n",
    "    j = 0\n",
    "    n = len(postings1)\n",
    "    m = len(postings2)\n",
    "    mergedPostings = []\n",
    "    comparisons = 0\n",
    "    \n",
    "    while i < n and j < m:\n",
    "        comparisons += 1\n",
    "    \n",
    "        if postings1[i] == postings2[j]:\n",
    "            i += 1\n",
    "            j += 1\n",
    "        elif postings1[i] < postings2[j]:\n",
    "            mergedPostings.append(postings1[i])\n",
    "            i += 1\n",
    "        else:\n",
    "            j += 1\n",
    "    \n",
    "    while i < n:\n",
    "        mergedPostings.append(postings1[i])\n",
    "        i += 1\n",
    "    \n",
    "    return mergedPostings, comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing OR NOT operation\n",
    "\n",
    "def ORNOT(postings1, postings2):\n",
    "    \n",
    "    notPostings2, notComparisons = NOT(postings2)\n",
    "    answer, orComparisons = OR(postings1, notPostings2)\n",
    "    \n",
    "    return answer, notComparisons + orComparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing Query Left to Right\n",
    "\n",
    "operations = {'OR': OR,'AND': AND,'OR NOT': ORNOT,'AND NOT': ANDNOT}\n",
    "\n",
    "def LeftToRightQuery(queryWords, queryOperations):\n",
    "    \n",
    "    global postings\n",
    "    queryPostings = []\n",
    "\n",
    "    if len(queryWords)==0:\n",
    "        return [],0\n",
    "    \n",
    "    if len(queryOperations)==0:\n",
    "        if queryWords[0] in postings:\n",
    "            return postings[queryWords[0]], 0\n",
    "        else:\n",
    "            return [], 0\n",
    "    \n",
    "    for word in queryWords:\n",
    "        if word in postings:\n",
    "            queryPostings.append(postings[word])\n",
    "        else:\n",
    "            queryPostings.append([])\n",
    "\n",
    "    answer = queryPostings[0]\n",
    "    totalComparisons = 0\n",
    "    comparisons = 0\n",
    "    \n",
    "    for i in range(len(queryOperations)):\n",
    "        answer, comparisons = operations[queryOperations[i]](answer, queryPostings[i + 1])\n",
    "        totalComparisons += comparisons\n",
    "    \n",
    "    return answer, totalComparisons    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the number of queries: 2\n",
      "Enter query sentence: lion stood thoughtfully for a moment\n",
      "Enter query operators: OR, OR, OR\n",
      "Number of documents matched:  270\n",
      "Number of comparisons required:  671\n",
      "Documents:\n",
      " ['13chil.txt', '14.lws', '16.lws', '18.lws', '19.lws', '20.lws', '3gables.txt', '3lpigs.txt', '3student.txt', '4moons.txt', '5orange.txt', '6napolen.txt', '7voysinb.txt', 'ab40thv.txt', 'abbey.txt', 'adv_alad.txt', 'advsayed.txt', 'aesop11.txt', 'aesopa10.txt', 'aisle.six', 'aislesix.txt', 'alad10.txt', 'aluminum.hum', 'angelfur.hum', 'angry_ca.txt', 'aquith.txt', 'arcadia.sty', 'archive', 'arctic.txt', 'beast.asc', 'beautbst.txt', 'beggars.txt', 'bestwish', 'beyond.hum', 'bgcspoof.txt', 'bishop00.txt', 'blabnove.hum', 'blabnove.txt', 'blackp.txt', 'blak', 'blasters.fic', 'blh.txt', 'blind.txt', 'blue', 'bluebrd.txt', 'bookem.1', 'bookem2', 'bookem3', 'brain.damage', 'bran', 'breaks1.asc', 'breaks2.asc', 'breaks3.asc', 'bruce-p.txt', 'buggy.txt', 'buldream.txt', 'bulfelis.txt', 'bulironb.txt', 'bulmrx.txt', 'bulolli1.txt', 'bulolli2.txt', 'bulphrek.txt', 'bulprint.txt', 'bulwer.lytton', 'bulzork1.txt', 'bureau.txt', 'burintrv.92', 'cardcnt.txt', 'chik', 'clon', 'cmoutmou.txt', 'consumdr.hum', 'cooldark.sto', 'cooldark.txt', 'corcor.hum', 'crazy.hum', 'cum', 'cybersla.txt', 'dan', 'darkness.txt', 'day.in.mcdonald', 'deal', 'deer.txt', 'descent.poe', 'diaryflf.txt', 'domain.poe', 'dopedenn.txt', 'dskool.txt', 'dtruck.txt', 'dwar', 'elite.app', 'emperor3.txt', 'empnclot.txt', 'empty.txt', 'enc', 'enchdup.hum', 'enginer.txt', 'enya_trn.txt', 'eyeargon.hum', 'ezoff', 'fantasy.hum', 'fantasy.txt', 'fgoose.txt', 'fic1', 'fic2', 'fic3', 'fic4', 'fic5', 'fic7', 'fish.txt', 'floc', 'floobs.txt', 'flytrunk.txt', 'forgotte', 'fourth.fic', 'foxncrow.txt', 'foxngrap.txt', 'fran', 'fred.txt', 'freeman.fil', 'friend.s', 'friends.txt', 'game.txt', 'ghost', 'girlclub.txt', 'gold3ber.txt', 'goldbug.poe', 'goldenp.txt', 'goldfish.txt', 'goldgoos.txt', 'grav', 'graymare.txt', 'greedog.txt', 'gulliver.txt', 'hareporc.txt', 'haretort.txt', 'healer.txt', 'hellmach.txt', 'hils', 'history5.txt', 'hitch2.txt', 'hitch3.txt', 'holmesbk.txt', 'home.fil', 'hop-frog.poe', 'horswolf.txt', 'hotline1.txt', 'hotline3.txt', 'hound-b.txt', 'idi.hum', 'igiv', 'imagin.hum', 'immortal', 'immorti.hum', 'inter', 'island.poe', 'jackbstl.txt', 'jackmac.fic', 'jerichms.hum', 'keepmodu.txt', 'knuckle.txt', 'korea.s', 'ladylust.hum', 'lament.txt', 'lgoldbrd.txt', 'lil', 'lionmane.txt', 'lionmosq.txt', 'lionwar.txt', 'lmtchgrl.txt', 'long1-3.txt', 'lrrhood.txt', 'ltp', 'lure.txt', 'mattress.txt', 'mazarin.txt', 'mcdonaldl.txt', 'missing.txt', 'monkking.txt', 'mouslion.txt', 'mtinder.txt', 'musgrave.txt', 'nigel.10', 'nigel.2', 'nigel.3', 'nigel.5', 'nihgel_8.9', 'nitepeek.sto', 'outcast.dos', 'paink-ws.txt', 'pepdegener.txt', 'pepsi.degenerat', 'perf', 'pinocch.txt', 'piracy.sto', 'plescopm.txt', 'poplstrm.txt', 'pphamlin.txt', 'pregn.txt', 'psf.txt', 'psi', 'pussboot.txt', 'qcarroll', 'quarter.c1', 'quarter.c13', 'quarter.c17', 'quarter.c18', 'quarter.c3', 'quest', 'radar_ra.txt', 'rainda.txt', 'reap', 'redragon.txt', 'retrib.txt', 'robotech', 'rock', 'rocket.sf', 'roger1.txt', 'running.txt', 's&m_plot', 's&m_that', 'shoscomb.txt', 'shulk.txt', 'sick-kid.txt', 'sight.txt', 'silverb.txt', 'sis', 'sleprncs.txt', 'snow.txt', 'snowmaid.txt', 'snowqn1.txt', 'social.vikings', 'socialvikings.txt', 'solitary.txt', 'space.txt', 'spam.key', 'spectacl.poe', 'spiders.txt', 'sqzply.txt', 'sre-dark.txt', 'sre04.txt', 'sre06.txt', 'sre07.txt', 'sre09.txt', 'sre10.txt', 'sre_finl.txt', 'srex.txt', 'stainles.ana', 'superg1', 'szechuan', 'taxnovel.txt', 'tcoa.txt', 'tearglas.txt', 'testpilo.hum', 'timem.hac', 'timetrav.txt', 'tree.txt', 'tuc_mees', 'unluckwr.txt', 'vainsong.txt', 'valen', 'vday.hum', 'veiledl.txt', 'vgilante.txt', 'wall.art', 'wisteria.txt', 'withdraw.cyb', 'wlgirl.txt', 'yukon.txt', 'zombies.txt']\n",
      "\n",
      "Enter query sentence: telephone,paved, roads\n",
      "Enter query operators: OR NOT, AND NOT\n",
      "Number of documents matched:  345\n",
      "Number of comparisons required:  1396\n",
      "Documents:\n",
      " ['14.lws', '17.lws', '18.lws', '19.lws', '20.lws', '3gables.txt', '3lpigs.txt', '3sonnets.vrs', '3student.txt', '3wishes.txt', '4moons.txt', '7oldsamr.txt', '7voysinb.txt', 'ab40thv.txt', 'adler.txt', 'adv_alad.txt', 'advsayed.txt', 'advtthum.txt', 'aircon.txt', 'aisle.six', 'aislesix.txt', 'alad10.txt', 'alissadl.txt', 'aminegg.txt', 'angelfur.hum', 'antcrick.txt', 'arcadia.sty', 'arctic.txt', 'asop', 'bagel.man', 'bagelman.txt', 'batlslau.txt', 'beast.asc', 'bern', 'berternie.txt', 'bestwish', 'beyond.hum', 'bgb.txt', 'bigred.hum', 'bishop00.txt', 'blabnove.hum', 'blabnove.txt', 'blackrdr', 'blak', 'blasters.fic', 'blh.txt', 'blossom.pom', 'blue', 'bluebrd.txt', 'bookem.1', 'bookem2', 'bookem3', 'brain.damage', 'bran', 'buggy.txt', 'buldetal.txt', 'buldream.txt', 'bulfelis.txt', 'bulhuntr.txt', 'bulironb.txt', 'bullove.txt', 'bulnland.txt', 'bulnoopt.txt', 'bulolli1.txt', 'bulolli2.txt', 'bulprint.txt', 'bulwer.lytton', 'bulzork1.txt', 'bumm', 'burintrv.66', 'burintrv.78', 'burintrv.92', 'burn', 'cameloto.hum', 'campfire.txt', 'cardcnt.txt', 'ccm.txt', 'chik', 'clevdonk.txt', 'clon', 'cmoutmou.txt', 'comp', 'confilct.fun', 'contrad1.hum', 'corcor.hum', 'cow.exploder', 'crabhern.txt', 'cum', 'curious.george', 'dakota.txt', 'dan', 'deal', 'deathmrs.d', 'descent.poe', 'dicegame.txt', 'dicksong.txt', 'disco.be.fun', 'discocanbefun.txt', 'domain.poe', 'dskool.txt', 'dwar', 'elite.app', 'elveshoe.txt', 'emperor3.txt', 'empnclot.txt', 'enc', 'encamp01.txt', 'excerpt.hum', 'excerpt.txt', 'eyeargon.hum', 'ezoff', 'fable.txt', 'fantas.hum', 'fea1', 'fea2', 'fea3', 'fear.hum', 'fearmnky', 'fic1', 'fic2', 'fic3', 'fic7', 'fish.txt', 'fleas.txt', 'flktrp.txt', 'floobs.txt', 'flute.txt', 'flytrunk.txt', 'fourth.fic', 'fowl.death', 'foxncrow.txt', 'foxngrap.txt', 'foxnstrk.txt', 'fred.txt', 'freeman.fil', 'friend.s', 'frogp.txt', 'frum', 'game.txt', 'gatherng.txt', 'gay', 'gemdra.txt', 'ghost', 'girl', 'girlclub.txt', 'glimpse1.txt', 'gloves.txt', 'gold3ber.txt', 'goldbug.poe', 'goldfish.txt', 'grav', 'graymare.txt', 'greatlrn.leg', 'greedog.txt', 'hansgrtl.txt', 'hareleph.txt', 'hareporc.txt', 'haretort.txt', 'healer.txt', 'hell4.txt', 'helmfuse.txt', 'hole2nar.txt', 'holmesbk.txt', 'home.fil', 'horsdonk.txt', 'horswolf.txt', 'hotline1.txt', 'hotline3.txt', 'hotline4.txt', 'how.ernie.bert', 'idi.hum', 'igiv', 'imagin.hum', 'immorti.hum', 'imonly17.txt', 'island.poe', 'jackbstl.txt', 'jackmac.fic', 'jaynejob.asc', 'jerichms.hum', 'jim.asc', 'keeping.insanit', 'keepmodu.txt', 'kharian.txt', 'kneeslapper', 'kneeslapper.txt', 'knuckle.txt', 'korea.s', 'kzap.txt', 'ladylust.hum', 'lgoldbrd.txt', 'life.txt', 'lil', 'lionbird', 'lionmosq.txt', 'lionwar.txt', 'lmermaid.txt', 'lpeargrl.txt', 'lrrhood.txt', 'ltp', 'luf', 'lure.txt', 'mario.txt', 'mazarin.txt', 'mike.txt', 'mindprob.txt', 'mindwar', 'modemhippy.txt', 'monkking.txt', 'monksol.txt', 'mouslion.txt', 'mtinder.txt', 'mydream.txt', 'myeyes', 'narciss.txt', 'nigel.10', 'nigel.2', 'nigel.4', 'nigel.7', 'non2', 'non3', 'non4', 'obstgoat.txt', 'omarsheh.txt', 'oxfrog.txt', 'paink-ws.txt', 'panama.txt', 'parotsha.txt', 'paul_har.sto', 'peace.fun', 'pepdegener.txt', 'pepsi.degenerat', 'pinocch.txt', 'piracy.sto', 'plescopm.txt', 'poem-1.txt', 'poem-2.txt', 'poem-4.txt', 'poplstrm.txt', 'pphamlin.txt', 'pregn.txt', 'prince.art', 'progx', 'psi', 'psyc', 'pussboot.txt', 'qcarroll', 'quarter.c10', 'quarter.c11', 'quarter.c12', 'quarter.c13', 'quarter.c14', 'quarter.c15', 'quarter.c16', 'quarter.c17', 'quarter.c19', 'quarter.c2', 'quarter.c3', 'quarter.c4', 'quarter.c5', 'quarter.c6', 'quarter.c7', 'quarter.c8', 'quarter.c9', 'quest', 'quickfix', 'quot', 'rainda.txt', 'redragon.txt', 'retrib.txt', 'rid.txt', 'rocket.sf', 'running.txt', 's&m_plot', 's&m_that', 'sanpedr2.txt', 'shrdfarm.txt', 'shulk.txt', 'sis', 'sleprncs.txt', 'snowmaid.txt', 'snowqn1.txt', 'space.txt', 'spam.key', 'spectacl.poe', 'sqzply.txt', 'sre01.txt', 'sre02.txt', 'sre03.txt', 'sre04.txt', 'sre05.txt', 'sre07.txt', 'sre08.txt', 'sre09.txt', 'sre10.txt', 'sre_feqh.txt', 'sre_finl.txt', 'sre_sei.txt', 'sretrade.txt', 'srex.txt', 'stainles.ana', 'stairdre.txt', 'startrek.txt', 'stsgreek', 'sucker.txt', 'sunday.txt', 'superg1', 'szechuan', 't_zone.jok', 'tailbear.txt', 'tao3.dos', 'tcoa.txt', 'tctac.txt', 'tearglas.txt', 'telefone.txt', 'terrorbears.txt', 'testpilo.hum', 'textfile.primer', 'thanksg', 'the-tree.txt', 'thewave', 'timem.hac', 'times.fic', 'timetrav.txt', 'tin', 'tinsoldr.txt', 'toilet.s', 'traitor.txt', 'tuc_mees', 'uglyduck.txt', 'vaincrow.txt', 'vainsong.txt', 'vampword.txt', 'wall.art', 'wanderer.fun', 'weaver.txt', 'weeprncs.txt', 'whgdsreg.reg', 'withdraw.cyb', 'wlgirl.txt', 'wolf7kid.txt', 'wolfcran.txt', 'wolflamb.txt', 'write', 'wrt']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Handling Input\n",
    "\n",
    "N = int(input(\"Enter the number of queries: \"))\n",
    "\n",
    "for n in range(N):\n",
    "    \n",
    "    Input_1 = input(\"Enter query sentence: \").strip()\n",
    "    queryWords = clean(Input_1).split()\n",
    "    \n",
    "    Input_2 = list(map(str,input(\"Enter query operators: \").split(', ')))\n",
    "    queryOperation = []\n",
    "    \n",
    "    for i in Input_2:\n",
    "        if i in operations:\n",
    "            queryOperation.append(i)\n",
    "    \n",
    "    if len(queryWords)-len(queryOperation) == 1:    \n",
    "        answer, comparisons = LeftToRightQuery(queryWords, queryOperation)\n",
    "    else:\n",
    "        print(\"Error\")\n",
    "    \n",
    "    print(\"Number of documents matched: \", len(answer))\n",
    "    print(\"Number of comparisons required: \", comparisons)\n",
    "    print(\"Documents:\\n\",[docID[i] for i in answer])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Processing Query in Efficient Order with Precedence\n",
    "\n",
    "# def QueryWithPrecedence(queryWords, queryOperation):\n",
    "    \n",
    "#     global postings\n",
    "#     queryPostings = []\n",
    "#     totalOperations = 0\n",
    "    \n",
    "#     for word in queryWords:\n",
    "#         if word in postings:\n",
    "#             queryPostings.append(postings[word])\n",
    "#         else:\n",
    "#             queryPostings.append([])\n",
    "    \n",
    "#     if len(queryOperation) == 1 and queryOperation[0] == \"AND NOT\":\n",
    "#         return ANDNOT(queryPostings[0], queryPostings[1])\n",
    "        \n",
    "#     for i in range(len(queryOperation)):\n",
    "    \n",
    "#         if queryOperation[i] == \"OR NOT\":\n",
    "#             queryPostings[i + 1], operations = NOT(queryPostings[i + 1])\n",
    "#             totalOperations += operations\n",
    "#             queryOperation[i] = \"OR\"\n",
    "        \n",
    "#         elif queryOperation[i] == \"AND NOT\":\n",
    "#             queryPostings[i + 1], operations = NOT(queryPostings[i + 1])\n",
    "#             totalOperations += operations\n",
    "#             queryOperation[i] = \"AND\"\n",
    "    \n",
    "#     newPostings = []\n",
    "#     i = 0\n",
    "    \n",
    "#     while i < len(queryWords):\n",
    "#         currPostings = [queryPostings[i]]\n",
    "    \n",
    "#         while i < len(queryOperation) and queryOperation[i] == \"AND\":\n",
    "#             currPostings.append(queryPostings[i + 1])\n",
    "#             i += 1\n",
    "        \n",
    "#         currPostings.sort(key = len)\n",
    "#         combinedPosting = currPostings[0]\n",
    "        \n",
    "#         for j in range(1, len(currPostings)):\n",
    "#             combinedPosting, operations = AND(combinedPosting, currPostings[j])\n",
    "#             totalOperations += operations\n",
    "        \n",
    "#         newPostings.append(combinedPosting)\n",
    "#         i += 1\n",
    "    \n",
    "#     newPostings.sort(key = len)\n",
    "#     combinedPosting = newPostings[0]\n",
    "    \n",
    "#     for i in range(1, len(newPostings)):    \n",
    "#         combinedPosting, operations = OR(combinedPosting, newPostings[i])\n",
    "#         totalOperations += operations\n",
    "    \n",
    "#     return combinedPosting, totalOperations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
