{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Positional Index\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing \n",
    "---\n",
    "The index.html files are removed and 15 files from the SRE folder are shifted to stories folder. The resulting 467 files are preprocessed before creating the Positional Index. Following steps are undertaken to clean the document text:-\n",
    "\n",
    "- Convert the text to lower case\n",
    "- Perform word tokenization\n",
    "- Remove punctuation marks from tokens\n",
    "- Remove stopwords from tokens\n",
    "- Remove blank space tokens\n",
    "\n",
    "The documents' names, original texts and cleaned texts obtained after preprocessing are stored in a pickle file **stories_data.pkl**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing relevent modules for preprocessing\n",
    "\n",
    "import re\n",
    "import codecs\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict, Counter \n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Documents : 467\n",
      "Unresolved Documents : []\n"
     ]
    }
   ],
   "source": [
    "# Reading the 467 documents and storing the document name and document text\n",
    "\n",
    "Data = []\n",
    "Unresolved = []\n",
    "\n",
    "for i in Path(\"../Other/stories\").glob(\"*\"):\n",
    "    try:\n",
    "        with codecs.open(i,'r', encoding = 'utf-8', errors = 'ignore') as f:\n",
    "            d = str(i).split('/')[-1]\n",
    "            t =  ' '.join(f.readlines())\n",
    "            Data.append({'doc_name' : d, 'text' : t})\n",
    "    except:\n",
    "        Unresolved.append(str(i))\n",
    "        \n",
    "print(\"Total Documents :\",len(Data))\n",
    "print(\"Unresolved Documents :\",Unresolved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean text\n",
    "\n",
    "def clean(text):\n",
    "    \n",
    "    # Converting text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Word Tokenization\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Removing all punctuation and unecessary characters from text\n",
    "    for i in range(len(words)):\n",
    "        words[i] = re.sub(r'[^a-z\\s]',' ',words[i])\n",
    "    \n",
    "    # Removing stopwords from text\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    \n",
    "    # Removing blank spaces\n",
    "    for i in range(len(words)):\n",
    "        words[i]=words[i].strip()\n",
    "        \n",
    "    words = [i for i in words if i!='']\n",
    "    \n",
    "    cleaned_text = ' '.join(words)\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 467/467 [00:25<00:00, 17.96it/s]\n"
     ]
    }
   ],
   "source": [
    "# Cleaning\n",
    "\n",
    "for doc in tqdm(Data):\n",
    "    doc['cleaned_text'] = clean(doc['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting documents in Alphabetical order\n",
    "\n",
    "Data.sort(key = lambda x:x['doc_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resolving documents\n",
    "\n",
    "docs = {}\n",
    "\n",
    "for i in range(len(Data)):\n",
    "    docs[i]={}\n",
    "    docs[i]['name']= Data[i]['doc_name']\n",
    "    docs[i]['size']= len(Data[i]['cleaned_text'].split())\n",
    "    docs[i]['max_frequency'] = Counter(Data[i]['cleaned_text'].split()).most_common(1)[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dumping in Pickle File\n",
    "\n",
    "pickle.dump(Data,open('../Dumps/stories_data.pkl','wb'))\n",
    "pickle.dump(docs,open('../Dumps/stories_docs.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Creating Positional Index\n",
    "---\n",
    "For creating the index we iterate through the cleaned document texts and store the document name as well as the position of the word in the document in the posting list of the term. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Cleaned Data\n",
    "\n",
    "Data = pickle.load(open('../Dumps/stories_data.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Index\n",
    "\n",
    "index = {}\n",
    "for doc in Data:\n",
    "    for i,term in enumerate(doc['cleaned_text'].split()):\n",
    "        if term in index:\n",
    "            index[term].append((doc['doc_name'],i))\n",
    "        else:\n",
    "            index[term] = [(doc['doc_name'],i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dumping in Pickle File\n",
    "\n",
    "pickle.dump(index,open('../Dumps/positional_index.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Query System   \n",
    "---\n",
    "For query system, we first load the index from the saved pickle file. Then we create pointers for all the words in the query and try to find the documents which contain all the words of the query in the correct order using the positional index created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Index\n",
    "\n",
    "index = pickle.load(open('../Dumps/positional_index.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phrase Query Processing\n",
    "\n",
    "def phrase_query(index,query):\n",
    "    \n",
    "    cleaned_query = clean(query).split()\n",
    "    m = len(cleaned_query)\n",
    "    pointers = [0 for i in range(m)]\n",
    "    answer = []\n",
    "    flag = True\n",
    "    \n",
    "    for i in range(m):\n",
    "        if cleaned_query[i] not in index:\n",
    "            flag = False\n",
    "            break\n",
    "            \n",
    "    test = 0\n",
    "    \n",
    "    while flag:\n",
    "        \n",
    "        test += 1\n",
    "        for i in range(m):\n",
    "            if pointers[i] == len(index[cleaned_query[i]]):\n",
    "                flag = False\n",
    "                break\n",
    "                \n",
    "        if flag == False:\n",
    "            break\n",
    "\n",
    "        for i in range(1,m):\n",
    "            if index[cleaned_query[i]][pointers[i]][0] != index[cleaned_query[0]][pointers[0]][0]:\n",
    "                flag = False\n",
    "                break\n",
    "        \n",
    "        if flag:\n",
    "            for i in range(1,m):\n",
    "                if index[cleaned_query[i]][pointers[i]][1] - index[cleaned_query[i-1]][pointers[i-1]][1] != 1:\n",
    "                    flag = False\n",
    "                    break\n",
    "            if flag:\n",
    "                answer.append(index[cleaned_query[0]][pointers[0]][0])\n",
    "        \n",
    "        j = 0\n",
    "        for i in range(1,m):\n",
    "            if index[cleaned_query[j]][pointers[j]] > index[cleaned_query[i]][pointers[i]]:\n",
    "                j = i\n",
    "        pointers[j] += 1\n",
    "        flag = True\n",
    "    unique = {}\n",
    "    for i in answer:\n",
    "        unique[i] = 1\n",
    "    return list(unique.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number Of Documents Retrieved: 21\n",
      "List Of Documents Retrieved:- \n",
      "\n",
      "13chil.txt\n",
      "aesop11.txt\n",
      "aesopa10.txt\n",
      "brain.damage\n",
      "breaks2.asc\n",
      "bruce-p.txt\n",
      "enchdup.hum\n",
      "fantasy.hum\n",
      "fantasy.txt\n",
      "fic5\n",
      "forgotte\n",
      "history5.txt\n",
      "horswolf.txt\n",
      "hound-b.txt\n",
      "mazarin.txt\n",
      "melissa.txt\n",
      "outcast.dos\n",
      "sick-kid.txt\n",
      "srex.txt\n",
      "startrek.txt\n",
      "superg1\n"
     ]
    }
   ],
   "source": [
    "query = \"good day\"\n",
    "results = phrase_query(index,query)\n",
    "print('Number Of Documents Retrieved:',len(results))\n",
    "print('List Of Documents Retrieved:- \\n')\n",
    "for i in results:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
