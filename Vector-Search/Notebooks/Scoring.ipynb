{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Scoring and Term-Weighting\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing relevant modules\n",
    "\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from math import log\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Execution Mode ###\n",
    "# 0 : Build Matrices & Setup\n",
    "# 1 : Testing\n",
    "# 2 : Interactive\n",
    "\n",
    "Flag = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Preprocessed Stories Data\n",
    "\n",
    "Data = pickle.load(open('../Dumps/stories_data.pkl','rb'))\n",
    "Docs = pickle.load(open('../Dumps/stories_docs.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean text\n",
    "\n",
    "def clean(text):\n",
    "    \n",
    "    # Converting text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Word Tokenization\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Removing all punctuation and unecessary characters from text\n",
    "    for i in range(len(words)):\n",
    "        words[i] = re.sub(r'[^a-z\\s]','',words[i])\n",
    "    \n",
    "    # Removing stopwords from text\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    \n",
    "    # Removing blank spaces\n",
    "    for i in range(len(words)):\n",
    "        words[i]=words[i].strip()\n",
    "        \n",
    "    words = [i for i in words if i!='']\n",
    "    \n",
    "    cleaned_text = ' '.join(words)\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Index & Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Index for TF-IDF\n",
    "\n",
    "if Flag == 0:\n",
    "    Index = {}\n",
    "    for i in tqdm(range(len(Data))):\n",
    "        words = Data[i]['cleaned_text'].split()\n",
    "        for j in words:\n",
    "            if j in Index:\n",
    "                if i in Index[j]:\n",
    "                    Index[j][i]+=1\n",
    "                else:\n",
    "                    Index[j][i]=1\n",
    "            else:\n",
    "                Index[j]={}\n",
    "                Index[j][i]=1\n",
    "\n",
    "    Vocabulary = list(Index.keys())\n",
    "    Vocabulary.sort()\n",
    "    pickle.dump(Vocabulary,open('../Dumps/tf_idf_vocabulary.pkl','wb'))\n",
    "    pickle.dump(Index,open('../Dumps/tf_idf_index.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Weighting Schemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Weighting Scheme\n",
    "\n",
    "def tf_idf_binary(word,doc):\n",
    "\n",
    "    if doc in Index[word]: \n",
    "        tf = 1\n",
    "    else: \n",
    "        tf = 0\n",
    "        \n",
    "    idf = log((len(Docs)/len(Index[word])+1),10)\n",
    "    \n",
    "    score = tf*idf\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw count Weighting Scheme\n",
    "\n",
    "def tf_idf_raw(word,doc):\n",
    "    \n",
    "    if doc in Index[word]: \n",
    "        tf = Index[word][doc]\n",
    "    else : \n",
    "        tf = 0\n",
    "        \n",
    "    idf = log((len(Docs)/len(Index[word])+1),10)\n",
    "    \n",
    "    score = tf*idf\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Term Frequency Weighting Scheme\n",
    "\n",
    "def tf_idf_frequency(word,doc):\n",
    "    \n",
    "    if doc in Index[word]: \n",
    "        tf = Index[word][doc]/Docs[doc]['size']\n",
    "    else : \n",
    "        tf = 0\n",
    "        \n",
    "    idf = log((len(Docs)/len(Index[word])+1),10)\n",
    "    \n",
    "    score = tf*idf\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log Normalization Weighting Scheme\n",
    "\n",
    "def tf_idf_log(word,doc):\n",
    "    \n",
    "    if doc in Index[word]: \n",
    "        tf = log(1+Index[word][doc])\n",
    "    else: \n",
    "        tf = 0\n",
    "        \n",
    "    idf = log((len(Docs)/len(Index[word])+1),10)\n",
    "    \n",
    "    score = tf*idf\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double Normalization Weighting Scheme\n",
    "\n",
    "def tf_idf_double(word,doc):\n",
    "\n",
    "    if doc in Index[word]:\n",
    "        tf = 0.5 + 0.5 * (Index[word][doc]/Docs[doc]['max_frequency'])\n",
    "    else: \n",
    "        tf = 0\n",
    "        \n",
    "    idf = log((len(Docs)/len(Index[word])+1),10)\n",
    "    \n",
    "    score = tf*idf\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building TF-IDF Score Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building Matrix\n",
    "\n",
    "if Flag == 0:\n",
    "    binary_matrix = [[0 for i in range(len(Vocabulary))] for j in range(len(Docs))]\n",
    "    raw_count_matrix = [[0 for i in range(len(Vocabulary))] for j in range(len(Docs))]\n",
    "    term_frequency_matrix = [[0 for i in range(len(Vocabulary))] for j in range(len(Docs))]\n",
    "    log_normalization_matrix = [[0 for i in range(len(Vocabulary))] for j in range(len(Docs))]\n",
    "    double_normalization_matrix = [[0 for i in range(len(Vocabulary))] for j in range(len(Docs))]\n",
    "\n",
    "    for x in tqdm(range(len(Docs))):\n",
    "        for y in range(len(Vocabulary)):\n",
    "            binary_matrix[x][y]= tf_idf_binary(Vocabulary[y],x)\n",
    "            raw_count_matrix[x][y]= tf_idf_raw(Vocabulary[y],x)\n",
    "            term_frequency_matrix[x][y]= tf_idf_frequency(Vocabulary[y],x)\n",
    "            log_normalization_matrix[x][y]= tf_idf_log(Vocabulary[y],x)\n",
    "            double_normalization_matrix[x][y]= tf_idf_double(Vocabulary[y],x)\n",
    "\n",
    "    pickle.dump(np.array(binary_matrix),open('../Dumps/binary_matrix.pkl','wb'))\n",
    "    pickle.dump(np.array(raw_count_matrix),open('../Dumps/raw_count_matrix.pkl','wb'))\n",
    "    pickle.dump(np.array(term_frequency_matrix),open('../Dumps/term_frequency_matrix.pkl','wb'))\n",
    "    pickle.dump(np.array(log_normalization_matrix),open('../Dumps/log_normalization_matrix.pkl','wb'))\n",
    "    pickle.dump(np.array(double_normalization_matrix),open('../Dumps/double_normalization_matrix.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Query System\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Pickle Files\n",
    "\n",
    "Index = pickle.load(open('../Dumps/tf_idf_index.pkl','rb'))\n",
    "Vocabulary = pickle.load(open('../Dumps/tf_idf_vocabulary.pkl','rb'))\n",
    "binary = pickle.load(open('../Dumps/binary_matrix.pkl','rb'))\n",
    "raw_count = pickle.load(open('../Dumps/raw_count_matrix.pkl','rb'))\n",
    "term_frequency = pickle.load(open('../Dumps/term_frequency_matrix.pkl','rb'))\n",
    "log_normalization = pickle.load(open('../Dumps/log_normalization_matrix.pkl','rb'))\n",
    "double_normalization = pickle.load(open('../Dumps/double_normalization_matrix.pkl','rb'))\n",
    "Weight_Modes = {1:binary,2:raw_count,3:term_frequency,4:log_normalization,5:double_normalization}\n",
    "Weight_Names = {1:'Binary',2:'Raw Count',3:'Term Frequency',4:'Log Normalization',5:'Double Normalization'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Scoring\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF_IDF of Query\n",
    "\n",
    "def tf_idf_query(query,mode):\n",
    "    query_vec = [0]*len(Vocabulary)\n",
    "    query_words = clean(query).split()\n",
    "    \n",
    "    for i in query_words:\n",
    "        if i in Vocabulary:\n",
    "            query_vec[Vocabulary.index(i)]+=1\n",
    "    \n",
    "    if mode == 1:\n",
    "        for i in range(len(query_vec)):\n",
    "            query_vec[i]= int(query_vec[i]>0)\n",
    "            \n",
    "    elif mode == 2:\n",
    "        for i in range(len(query_vec)):\n",
    "            query_vec[i]= query_vec[i]\n",
    "            \n",
    "    elif mode == 3:\n",
    "        S = len(query_words)\n",
    "        for i in range(len(query_vec)):\n",
    "            query_vec[i]/= S\n",
    "            \n",
    "    elif mode == 4:\n",
    "        for i in range(len(query_vec)):\n",
    "            query_vec[i]= log(1+query_vec[i],10)\n",
    "            \n",
    "    elif mode == 5:\n",
    "        M = max(query_vec)\n",
    "        for i in range(len(query_vec)):\n",
    "            query_vec[i]= 0.5*int(query_vec[i]>0) + 0.5*(query_vec[i]/M)\n",
    "    \n",
    "    for i in range(len(query_vec)):\n",
    "        query_vec[i]*=log((len(Docs)/len(Index[Vocabulary[i]])+1),10) \n",
    "        \n",
    "    query_vec = np.array(query_vec).reshape(len(query_vec),1)\n",
    "    \n",
    "    return query_vec  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to handle TF_IDF Query\n",
    "\n",
    "def tf_idf_scoring(query,mode,qw=0):\n",
    "    \n",
    "    matrix = Weight_Modes[mode]\n",
    "    query_vec = tf_idf_query(query,mode)\n",
    "    if qw : query_vec = tf_idf_query(query,qw)\n",
    "    scores = matrix.dot(query_vec)\n",
    "    result = [(scores[i][0],i) for i in range(len(scores))]\n",
    "    result = sorted(result,reverse=True)[:5]\n",
    "    \n",
    "    for i in result:\n",
    "        print(Docs[i[1]]['name']+' : '+str(i[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to handle Cosine Similarity Query\n",
    "\n",
    "def cosine_similarity_scoring(query,mode,qw=0): \n",
    "    \n",
    "    matrix = Weight_Modes[mode]\n",
    "    query_vec = tf_idf_query(query,mode)\n",
    "    if qw : query_vec = tf_idf_query(query,qw)\n",
    "    scores = matrix.dot(query_vec)\n",
    "    \n",
    "    scores = scores/np.linalg.norm(query_vec)\n",
    "    for i in range(len(matrix)): scores[i] = scores[i]/np.linalg.norm(matrix[i])\n",
    "\n",
    "    result = [(scores[i][0],i) for i in range(len(scores))]\n",
    "    result = sorted(result,reverse=True)[:5]\n",
    "    \n",
    "    for i in result:\n",
    "        print(Docs[i[1]]['name']+' : '+str(i[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccard Coefficient\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate Jaccard Coefficient\n",
    "\n",
    "def jaccard_coeff(doc,query):\n",
    "    query_set = query.split(\" \")\n",
    "    doc_set = doc.split(\" \")\n",
    "    intersection = len(list(set(query_set) & set(doc_set)))\n",
    "    union = len(list(set(query_set) | set(doc_set)))\n",
    "    coeff = intersection/union\n",
    "    return coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to handle Jaccard Coefficient Query\n",
    "\n",
    "def jaccard_scoring(query):\n",
    "    \n",
    "    scores = []\n",
    "    query = clean(query)\n",
    "    \n",
    "    for i in Docs:\n",
    "        doc = Data[i]['cleaned_text']\n",
    "        s = jaccard_coeff(doc,query)\n",
    "        scores.append((s,i))\n",
    "\n",
    "    result = [(scores[i][0],i) for i in range(len(scores))]\n",
    "    result = sorted(scores,reverse=True)[:5]\n",
    "    \n",
    "    for i in result:\n",
    "        print(Docs[i[1]]['name']+' : '+str(i[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "TF-IDF\n",
      "---------------------------------------------------------------------------\n",
      ">> Binary\n",
      "crabhern.txt : 9.518091493689194\n",
      "aesop11.txt : 9.518091493689194\n",
      "timem.hac : 3.900514378261692\n",
      "long1-3.txt : 3.900514378261692\n",
      "fgoose.txt : 3.900514378261692\n",
      "---------------------------------------------------------------------------\n",
      ">> Raw Count\n",
      "crabhern.txt : 89.56333782146444\n",
      "aesop11.txt : 29.020663384997654\n",
      "timem.hac : 7.801028756523384\n",
      "long1-3.txt : 7.801028756523384\n",
      "fgoose.txt : 3.900514378261692\n",
      "---------------------------------------------------------------------------\n",
      ">> Term Frequency\n",
      "crabhern.txt : 0.2344589995326294\n",
      "aesop11.txt : 0.0007679049371559497\n",
      "long1-3.txt : 0.0006011890225434175\n",
      "fgoose.txt : 0.00028828635463870595\n",
      "timem.hac : 0.00025341179692448624\n",
      "---------------------------------------------------------------------------\n",
      ">> Log Normalization\n",
      "crabhern.txt : 6.709348811055275\n",
      "aesop11.txt : 3.4569858005074092\n",
      "timem.hac : 1.289959597463907\n",
      "long1-3.txt : 1.289959597463907\n",
      "fgoose.txt : 0.8138738909450393\n",
      "---------------------------------------------------------------------------\n",
      ">> Double Normalization\n",
      "crabhern.txt : 9.237212637917818\n",
      "aesop11.txt : 4.8268510351273015\n",
      "long1-3.txt : 1.9961455935809833\n",
      "fgoose.txt : 1.977725600245365\n",
      "timem.hac : 1.9701577726934054\n",
      "---------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------\n",
      "Cosine Similarity\n",
      "---------------------------------------------------------------------------\n",
      ">> Binary\n",
      "crabhern.txt : 0.23569842076317024\n",
      "aesop11.txt : 0.02519736103537911\n",
      "long1-3.txt : 0.01835059980869695\n",
      "fgoose.txt : 0.015732510863531078\n",
      "timem.hac : 0.011950161927944092\n",
      "---------------------------------------------------------------------------\n",
      ">> Raw Count\n",
      "crabhern.txt : 0.8321154761836865\n",
      "aesop11.txt : 0.016422015288890562\n",
      "long1-3.txt : 0.008820584108021872\n",
      "timem.hac : 0.006187701310915087\n",
      "fgoose.txt : 0.0055847175566704245\n",
      "---------------------------------------------------------------------------\n",
      ">> Term Frequency\n",
      "crabhern.txt : 0.8321154761836863\n",
      "aesop11.txt : 0.016422015288890562\n",
      "long1-3.txt : 0.008820584108021874\n",
      "timem.hac : 0.0061877013109150865\n",
      "fgoose.txt : 0.005584717556670424\n",
      "---------------------------------------------------------------------------\n",
      ">> Log Normalization\n",
      "crabhern.txt : 0.5866105872226275\n",
      "aesop11.txt : 0.028334772283310074\n",
      "long1-3.txt : 0.0207366748066286\n",
      "timem.hac : 0.013215085467333965\n",
      "fgoose.txt : 0.01131858945051863\n",
      "---------------------------------------------------------------------------\n",
      ">> Double Normalization\n",
      "crabhern.txt : 0.38631036507021665\n",
      "aesop11.txt : 0.02529853999520121\n",
      "long1-3.txt : 0.01837149368233129\n",
      "fgoose.txt : 0.015573688679658058\n",
      "timem.hac : 0.011956896050323434\n",
      "---------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------\n",
      "Jaccard Coefficient\n",
      "---------------------------------------------------------------------------\n",
      "crabhern.txt : 0.013986013986013986\n",
      "long1-3.txt : 0.00041893590280687055\n",
      "aesop11.txt : 0.00038138825324180017\n",
      "fgoose.txt : 0.0003418803418803419\n",
      "timem.hac : 0.00022306491188935982\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "\n",
    "if Flag == 1:\n",
    "    \n",
    "    query = \"The crab and the heron\"\n",
    "\n",
    "    print(\"-\"*75)\n",
    "    print(\"TF-IDF\")\n",
    "    print(\"-\"*75)\n",
    "\n",
    "    for i in range(1,6):\n",
    "        print('>> '+Weight_Names[i])\n",
    "        tf_idf_scoring(query,i)\n",
    "        print(\"-\"*75)\n",
    "\n",
    "    print(\"-\"*75)\n",
    "    print(\"Cosine Similarity\")\n",
    "    print(\"-\"*75)\n",
    "\n",
    "    for i in range(1,6):\n",
    "        print('>> '+Weight_Names[i])\n",
    "        cosine_similarity_scoring(query,i)\n",
    "        print(\"-\"*75)\n",
    "\n",
    "    print(\"-\"*75)\n",
    "    print(\"Jaccard Coefficient\")\n",
    "    print(\"-\"*75)\n",
    "\n",
    "    jaccard_scoring(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query System\n",
    "\n",
    "if Flag == 2:\n",
    "    \n",
    "    print(\"-\"*75)\n",
    "    print(\"Choose a Scoring Scheme:-\")\n",
    "    print(\"-\"*75)\n",
    "    print(\"1: TF-IDF\")\n",
    "    print(\"2: Cosine Similarity\")\n",
    "    print(\"3: Jaccard Coefficient\")\n",
    "\n",
    "    n = int(input(\"Choice:\"))\n",
    "    if n == 1 or n == 2:\n",
    "        print(\"-\"*75)\n",
    "        print(\"Choose a Weighting Scheme:-\")\n",
    "        print(\"-\"*75)\n",
    "        print(\"1: Binary\")\n",
    "        print(\"2: Raw Count\")\n",
    "        print(\"3: Term Frequency\")\n",
    "        print(\"4: Log Normalization\")\n",
    "        print(\"5: Double Normalization\")\n",
    "        print(\"-\"*75)\n",
    "        m = int(input(\"Choice:\"))\n",
    "\n",
    "    print(\"-\"*75)\n",
    "    q = input(\"Enter Query: \")\n",
    "    print(\"-\"*75)\n",
    "\n",
    "    if n == 1:\n",
    "        tf_idf_scoring(q,m)\n",
    "    if n == 2:\n",
    "        cosine_similarity_scoring(q,m)\n",
    "    if n == 3:\n",
    "        jaccard_scoring(q)\n",
    "\n",
    "    print(\"-\"*75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
